{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "# import packages and modules for statistical analysis\n",
    "from scipy import stats\n",
    "import scipy.stats as scs\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import linear_model, metrics, tree\n",
    "\n",
    "# import modules for preprocessing\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_regression, RFECV, mutual_info_regression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler,OneHotEncoder\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import validation_curve, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def root_mean_squared_error(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "def bias(y, y_pred):\n",
    "\treturn np.mean(y_pred - y)\n",
    "def variance(y_pred):\n",
    "\treturn np.mean([yi**2 for yi in y_pred]) - np.mean(y_pred)**2\n",
    "\n",
    "\n",
    "# import module for object serialization\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Feature Engineering Continued\n",
    "\n",
    "## One-Hot Encoding/Dummy Variables\n",
    "\n",
    "Creating dummy variables allow us to input categorical variables into the Machine Learning models, which require that all input data be numerical. Here, they are numerical, but take on discrete numerical values, so we consider them as categorical.  Dummy variables only take on the value of 0 or 1 for the absence or presence of some aspect of the category that is expected to effect the outcome. We did not create polynomial and interaction features for dummy variables since the values are only 0 and 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
       "       'waterfront', 'condition', 'grade', 'sqft_above', 'zipcode', 'lat',\n",
       "       'long', 'sqft_living15', 'sqft_lot15', 'sale_age', 'age', 'renovated',\n",
       "       'basement', 'viewed'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Get index of the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   bdr_1  bdr_2  bdr_3  bdr_4  bdr_5  bdr_6  bdr_7  bdr_8  bdr_9  bdr_10  \\\n",
       "0      0      0      0      1      0      0      0      0      0       0   \n",
       "1      0      0      0      0      1      0      0      0      0       0   \n",
       "2      0      0      0      1      0      0      0      0      0       0   \n",
       "3      0      0      1      0      0      0      0      0      0       0   \n",
       "4      0      0      1      0      0      0      0      0      0       0   \n",
       "\n",
       "   bdr_11  bth_0.5  bth_0.75  bth_1.0  bth_1.25  bth_1.5  bth_1.75  bth_2.0  \\\n",
       "0       0        0         0        0         0        0         0        0   \n",
       "1       0        0         0        0         0        0         0        0   \n",
       "2       0        0         0        0         0        0         0        0   \n",
       "3       0        0         0        0         0        0         0        0   \n",
       "4       0        0         0        0         0        0         0        0   \n",
       "\n",
       "   bth_2.25  bth_2.5  bth_2.75  bth_3.0  bth_3.25  bth_3.5  bth_3.75  bth_4.0  \\\n",
       "0         1        0         0        0         0        0         0        0   \n",
       "1         0        0         0        1         0        0         0        0   \n",
       "2         0        1         0        0         0        0         0        0   \n",
       "3         0        0         0        0         0        1         0        0   \n",
       "4         0        1         0        0         0        0         0        0   \n",
       "\n",
       "   bth_4.25  bth_4.5  bth_4.75  bth_5.0  bth_5.25  bth_5.5  bth_5.75  bth_6.0  \\\n",
       "0         0        0         0        0         0        0         0        0   \n",
       "1         0        0         0        0         0        0         0        0   \n",
       "2         0        0         0        0         0        0         0        0   \n",
       "3         0        0         0        0         0        0         0        0   \n",
       "4         0        0         0        0         0        0         0        0   \n",
       "\n",
       "   bth_6.25  bth_6.5  bth_6.75  bth_7.5  bth_7.75  bth_8.0  flr_1.5  flr_2.0  \\\n",
       "0         0        0         0        0         0        0        0        1   \n",
       "1         0        0         0        0         0        0        0        0   \n",
       "2         0        0         0        0         0        0        0        1   \n",
       "3         0        0         0        0         0        0        0        1   \n",
       "4         0        0         0        0         0        0        0        1   \n",
       "\n",
       "   flr_2.5  flr_3.0  flr_3.5  cnd_2  cnd_3  cnd_4  cnd_5  grd_3  grd_4  grd_5  \\\n",
       "0        0        0        0      0      0      1      0      0      0      0   \n",
       "1        0        0        0      0      0      0      1      0      0      0   \n",
       "2        0        0        0      0      1      0      0      0      0      0   \n",
       "3        0        0        0      0      1      0      0      0      0      0   \n",
       "4        0        0        0      0      1      0      0      0      0      0   \n",
       "\n",
       "   grd_6  grd_7  grd_8  grd_9  grd_10  grd_11  grd_12  grd_13  zip_98002  \\\n",
       "0      0      0      1      0       0       0       0       0          0   \n",
       "1      0      0      1      0       0       0       0       0          0   \n",
       "2      0      0      0      0       0       1       0       0          0   \n",
       "3      0      0      0      0       0       0       1       0          0   \n",
       "4      0      0      0      1       0       0       0       0          0   \n",
       "\n",
       "   zip_98003  zip_98004  zip_98005  zip_98006  zip_98007  zip_98008  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          1          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98010  zip_98011  zip_98014  zip_98019  zip_98022  zip_98023  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98024  zip_98027  zip_98028  zip_98029  zip_98030  zip_98031  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98032  zip_98033  zip_98034  zip_98038  zip_98039  zip_98040  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          1          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98042  zip_98045  zip_98052  zip_98053  zip_98055  zip_98056  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          1          0          0          0   \n",
       "\n",
       "   zip_98058  zip_98059  zip_98065  zip_98070  zip_98072  zip_98074  \\\n",
       "0          1          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98075  zip_98077  zip_98092  zip_98102  zip_98103  zip_98105  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98106  zip_98107  zip_98108  zip_98109  zip_98112  zip_98115  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          1   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98116  zip_98117  zip_98118  zip_98119  zip_98122  zip_98125  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98126  zip_98133  zip_98136  zip_98144  zip_98146  zip_98148  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98155  zip_98166  zip_98168  zip_98177  zip_98178  zip_98188  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98198  zip_98199  \n",
       "0          0          0  \n",
       "1          0          0  \n",
       "2          0          0  \n",
       "3          0          0  \n",
       "4          0          0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bdr_1</th>\n      <th>bdr_2</th>\n      <th>bdr_3</th>\n      <th>bdr_4</th>\n      <th>bdr_5</th>\n      <th>bdr_6</th>\n      <th>bdr_7</th>\n      <th>bdr_8</th>\n      <th>bdr_9</th>\n      <th>bdr_10</th>\n      <th>bdr_11</th>\n      <th>bth_0.5</th>\n      <th>bth_0.75</th>\n      <th>bth_1.0</th>\n      <th>bth_1.25</th>\n      <th>bth_1.5</th>\n      <th>bth_1.75</th>\n      <th>bth_2.0</th>\n      <th>bth_2.25</th>\n      <th>bth_2.5</th>\n      <th>bth_2.75</th>\n      <th>bth_3.0</th>\n      <th>bth_3.25</th>\n      <th>bth_3.5</th>\n      <th>bth_3.75</th>\n      <th>bth_4.0</th>\n      <th>bth_4.25</th>\n      <th>bth_4.5</th>\n      <th>bth_4.75</th>\n      <th>bth_5.0</th>\n      <th>bth_5.25</th>\n      <th>bth_5.5</th>\n      <th>bth_5.75</th>\n      <th>bth_6.0</th>\n      <th>bth_6.25</th>\n      <th>bth_6.5</th>\n      <th>bth_6.75</th>\n      <th>bth_7.5</th>\n      <th>bth_7.75</th>\n      <th>bth_8.0</th>\n      <th>flr_1.5</th>\n      <th>flr_2.0</th>\n      <th>flr_2.5</th>\n      <th>flr_3.0</th>\n      <th>flr_3.5</th>\n      <th>cnd_2</th>\n      <th>cnd_3</th>\n      <th>cnd_4</th>\n      <th>cnd_5</th>\n      <th>grd_3</th>\n      <th>grd_4</th>\n      <th>grd_5</th>\n      <th>grd_6</th>\n      <th>grd_7</th>\n      <th>grd_8</th>\n      <th>grd_9</th>\n      <th>grd_10</th>\n      <th>grd_11</th>\n      <th>grd_12</th>\n      <th>grd_13</th>\n      <th>zip_98002</th>\n      <th>zip_98003</th>\n      <th>zip_98004</th>\n      <th>zip_98005</th>\n      <th>zip_98006</th>\n      <th>zip_98007</th>\n      <th>zip_98008</th>\n      <th>zip_98010</th>\n      <th>zip_98011</th>\n      <th>zip_98014</th>\n      <th>zip_98019</th>\n      <th>zip_98022</th>\n      <th>zip_98023</th>\n      <th>zip_98024</th>\n      <th>zip_98027</th>\n      <th>zip_98028</th>\n      <th>zip_98029</th>\n      <th>zip_98030</th>\n      <th>zip_98031</th>\n      <th>zip_98032</th>\n      <th>zip_98033</th>\n      <th>zip_98034</th>\n      <th>zip_98038</th>\n      <th>zip_98039</th>\n      <th>zip_98040</th>\n      <th>zip_98042</th>\n      <th>zip_98045</th>\n      <th>zip_98052</th>\n      <th>zip_98053</th>\n      <th>zip_98055</th>\n      <th>zip_98056</th>\n      <th>zip_98058</th>\n      <th>zip_98059</th>\n      <th>zip_98065</th>\n      <th>zip_98070</th>\n      <th>zip_98072</th>\n      <th>zip_98074</th>\n      <th>zip_98075</th>\n      <th>zip_98077</th>\n      <th>zip_98092</th>\n      <th>zip_98102</th>\n      <th>zip_98103</th>\n      <th>zip_98105</th>\n      <th>zip_98106</th>\n      <th>zip_98107</th>\n      <th>zip_98108</th>\n      <th>zip_98109</th>\n      <th>zip_98112</th>\n      <th>zip_98115</th>\n      <th>zip_98116</th>\n      <th>zip_98117</th>\n      <th>zip_98118</th>\n      <th>zip_98119</th>\n      <th>zip_98122</th>\n      <th>zip_98125</th>\n      <th>zip_98126</th>\n      <th>zip_98133</th>\n      <th>zip_98136</th>\n      <th>zip_98144</th>\n      <th>zip_98146</th>\n      <th>zip_98148</th>\n      <th>zip_98155</th>\n      <th>zip_98166</th>\n      <th>zip_98168</th>\n      <th>zip_98177</th>\n      <th>zip_98178</th>\n      <th>zip_98188</th>\n      <th>zip_98198</th>\n      <th>zip_98199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Grab indices of columns for creating dummy variables and create dataframe with dummy variables\n",
    "dum_feat = df[['bedrooms', 'bathrooms', 'floors', 'condition', 'grade', 'zipcode']]\n",
    "dum_index = dum_feat.columns\n",
    "# To prevent what they call the dummy variable trap (related to multicollinearity), drop one of the dummy variable, as well as  the original categorical variable used in creating the dummy variables\n",
    "df_dum = pd.get_dummies(data=dum_feat, columns=dum_index, drop_first=True, prefix=['bdr', 'bth', 'flr', 'cnd', 'grd', 'zip'])\n",
    "df_dum.head()"
   ]
  },
  {
   "source": [
    "## Polynomial and Interaction Features\n",
    "\n",
    "Polynomial features are created by raising our exisitng features by some exponent, generally not greater than 3 or 4.  Adding polynomial features helps the regression models to recognize nonlinear patterns. For instance, age is related to price in more of a parabolic function due to the higher premium placed on brand new constructions vs. vintage or historic homes, which are on opposite ends of the age spectrum.\n",
    "\n",
    "Interaction features, however, are represented by one variable or feature multipled by another feature. The idea here is that feature A's effect on C depend on the differing values of feature B.  Let's say C is plant growth, feature A is the amount of bacteria and feature B is the amount of sunlight.  In low amounts of sunlight, a high amount of bacteria in the soil creates tall plants, let's say, but in high amounts of sunlight, that same amount of bacteria creates short plants.  Only an interaction feature would be able to express that relationship."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   sqft_living  sqft_lot  waterfront  sqft_above      lat     long  \\\n",
       "0       2070.0    8893.0         0.0      2070.0  47.4388 -122.162   \n",
       "1       2900.0    6730.0         0.0      1830.0  47.6784 -122.285   \n",
       "2       3770.0   10893.0         0.0      3770.0  47.5646 -122.129   \n",
       "3       4560.0   14608.0         0.0      4560.0  47.6995 -122.228   \n",
       "4       2550.0    5376.0         0.0      2550.0  47.6647 -122.083   \n",
       "\n",
       "   sqft_living15  sqft_lot15  sale_age   age  renovated  basement  viewed  \\\n",
       "0         2390.0      7700.0      28.0  28.0        0.0       0.0     0.0   \n",
       "1         2370.0      6283.0      37.0  37.0        0.0       1.0     0.0   \n",
       "2         3710.0      9685.0      17.0  17.0        0.0       0.0     1.0   \n",
       "3         4050.0     14226.0      25.0  25.0        0.0       0.0     1.0   \n",
       "4         2250.0      4050.0      10.0  10.0        0.0       0.0     0.0   \n",
       "\n",
       "   sqft_living^2  sqft_living sqft_lot  sqft_living waterfront  \\\n",
       "0      4284900.0            18408510.0                     0.0   \n",
       "1      8410000.0            19517000.0                     0.0   \n",
       "2     14212900.0            41066610.0                     0.0   \n",
       "3     20793600.0            66612480.0                     0.0   \n",
       "4      6502500.0            13708800.0                     0.0   \n",
       "\n",
       "   sqft_living sqft_above  sqft_living lat  sqft_living long  \\\n",
       "0               4284900.0        98198.316        -252875.34   \n",
       "1               5307000.0       138267.360        -354626.50   \n",
       "2              14212900.0       179318.542        -460426.33   \n",
       "3              20793600.0       217509.720        -557359.68   \n",
       "4               6502500.0       121544.985        -311311.65   \n",
       "\n",
       "   sqft_living sqft_living15  sqft_living sqft_lot15  sqft_living sale_age  \\\n",
       "0                  4947300.0              15939000.0               57960.0   \n",
       "1                  6873000.0              18220700.0              107300.0   \n",
       "2                 13986700.0              36512450.0               64090.0   \n",
       "3                 18468000.0              64870560.0              114000.0   \n",
       "4                  5737500.0              10327500.0               25500.0   \n",
       "\n",
       "   sqft_living age  sqft_living renovated  sqft_living basement  \\\n",
       "0          57960.0                    0.0                   0.0   \n",
       "1         107300.0                    0.0                2900.0   \n",
       "2          64090.0                    0.0                   0.0   \n",
       "3         114000.0                    0.0                   0.0   \n",
       "4          25500.0                    0.0                   0.0   \n",
       "\n",
       "   sqft_living viewed   sqft_lot^2  sqft_lot waterfront  sqft_lot sqft_above  \\\n",
       "0                 0.0   79085449.0                  0.0           18408510.0   \n",
       "1                 0.0   45292900.0                  0.0           12315900.0   \n",
       "2              3770.0  118657449.0                  0.0           41066610.0   \n",
       "3              4560.0  213393664.0                  0.0           66612480.0   \n",
       "4                 0.0   28901376.0                  0.0           13708800.0   \n",
       "\n",
       "   sqft_lot lat  sqft_lot long  sqft_lot sqft_living15  sqft_lot sqft_lot15  \\\n",
       "0   421873.2484   -1086386.666              21254270.0           68476100.0   \n",
       "1   320875.6320    -822978.050              15950100.0           42284590.0   \n",
       "2   518121.1878   -1330351.197              40413030.0          105498705.0   \n",
       "3   696794.2960   -1785506.624              59162400.0          207813408.0   \n",
       "4   256245.4272    -656318.208              12096000.0           21772800.0   \n",
       "\n",
       "   sqft_lot sale_age  sqft_lot age  sqft_lot renovated  sqft_lot basement  \\\n",
       "0           249004.0      249004.0                 0.0                0.0   \n",
       "1           249010.0      249010.0                 0.0             6730.0   \n",
       "2           185181.0      185181.0                 0.0                0.0   \n",
       "3           365200.0      365200.0                 0.0                0.0   \n",
       "4            53760.0       53760.0                 0.0                0.0   \n",
       "\n",
       "   sqft_lot viewed  waterfront^2  waterfront sqft_above  waterfront lat  \\\n",
       "0              0.0           0.0                    0.0             0.0   \n",
       "1              0.0           0.0                    0.0             0.0   \n",
       "2          10893.0           0.0                    0.0             0.0   \n",
       "3          14608.0           0.0                    0.0             0.0   \n",
       "4              0.0           0.0                    0.0             0.0   \n",
       "\n",
       "   waterfront long  waterfront sqft_living15  waterfront sqft_lot15  \\\n",
       "0             -0.0                       0.0                    0.0   \n",
       "1             -0.0                       0.0                    0.0   \n",
       "2             -0.0                       0.0                    0.0   \n",
       "3             -0.0                       0.0                    0.0   \n",
       "4             -0.0                       0.0                    0.0   \n",
       "\n",
       "   waterfront sale_age  waterfront age  waterfront renovated  \\\n",
       "0                  0.0             0.0                   0.0   \n",
       "1                  0.0             0.0                   0.0   \n",
       "2                  0.0             0.0                   0.0   \n",
       "3                  0.0             0.0                   0.0   \n",
       "4                  0.0             0.0                   0.0   \n",
       "\n",
       "   waterfront basement  waterfront viewed  sqft_above^2  sqft_above lat  \\\n",
       "0                  0.0                0.0     4284900.0       98198.316   \n",
       "1                  0.0                0.0     3348900.0       87251.472   \n",
       "2                  0.0                0.0    14212900.0      179318.542   \n",
       "3                  0.0                0.0    20793600.0      217509.720   \n",
       "4                  0.0                0.0     6502500.0      121544.985   \n",
       "\n",
       "   sqft_above long  sqft_above sqft_living15  sqft_above sqft_lot15  \\\n",
       "0       -252875.34                 4947300.0             15939000.0   \n",
       "1       -223781.55                 4337100.0             11497890.0   \n",
       "2       -460426.33                13986700.0             36512450.0   \n",
       "3       -557359.68                18468000.0             64870560.0   \n",
       "4       -311311.65                 5737500.0             10327500.0   \n",
       "\n",
       "   sqft_above sale_age  sqft_above age  sqft_above renovated  \\\n",
       "0              57960.0         57960.0                   0.0   \n",
       "1              67710.0         67710.0                   0.0   \n",
       "2              64090.0         64090.0                   0.0   \n",
       "3             114000.0        114000.0                   0.0   \n",
       "4              25500.0         25500.0                   0.0   \n",
       "\n",
       "   sqft_above basement  sqft_above viewed        lat^2     lat long  \\\n",
       "0                  0.0                0.0  2250.439745 -5795.218686   \n",
       "1               1830.0                0.0  2273.229827 -5830.353144   \n",
       "2                  0.0             3770.0  2262.391173 -5809.017033   \n",
       "3                  0.0             4560.0  2275.242300 -5830.214486   \n",
       "4                  0.0                0.0  2271.923626 -5819.049570   \n",
       "\n",
       "   lat sqft_living15  lat sqft_lot15  lat sale_age    lat age  lat renovated  \\\n",
       "0         113378.732     365278.7600     1328.2864  1328.2864            0.0   \n",
       "1         112997.808     299563.3872     1764.1008  1764.1008            0.0   \n",
       "2         176464.666     460663.1510      808.5982   808.5982            0.0   \n",
       "3         193182.975     678573.0870     1192.4875  1192.4875            0.0   \n",
       "4         107245.575     193042.0350      476.6470   476.6470            0.0   \n",
       "\n",
       "   lat basement  lat viewed        long^2  long sqft_living15  \\\n",
       "0        0.0000      0.0000  14923.554244          -291967.18   \n",
       "1       47.6784      0.0000  14953.621225          -289815.45   \n",
       "2        0.0000     47.5646  14915.492641          -453098.59   \n",
       "3        0.0000     47.6995  14939.683984          -495023.40   \n",
       "4        0.0000      0.0000  14904.258889          -274686.75   \n",
       "\n",
       "   long sqft_lot15  long sale_age  long age  long renovated  long basement  \\\n",
       "0      -940647.400      -3420.536 -3420.536            -0.0         -0.000   \n",
       "1      -768316.655      -4524.545 -4524.545            -0.0       -122.285   \n",
       "2     -1182819.365      -2076.193 -2076.193            -0.0         -0.000   \n",
       "3     -1738815.528      -3055.700 -3055.700            -0.0         -0.000   \n",
       "4      -494436.150      -1220.830 -1220.830            -0.0         -0.000   \n",
       "\n",
       "   long viewed  sqft_living15^2  sqft_living15 sqft_lot15  \\\n",
       "0       -0.000        5712100.0                18403000.0   \n",
       "1       -0.000        5616900.0                14890710.0   \n",
       "2     -122.129       13764100.0                35931350.0   \n",
       "3     -122.228       16402500.0                57615300.0   \n",
       "4       -0.000        5062500.0                 9112500.0   \n",
       "\n",
       "   sqft_living15 sale_age  sqft_living15 age  sqft_living15 renovated  \\\n",
       "0                 66920.0            66920.0                      0.0   \n",
       "1                 87690.0            87690.0                      0.0   \n",
       "2                 63070.0            63070.0                      0.0   \n",
       "3                101250.0           101250.0                      0.0   \n",
       "4                 22500.0            22500.0                      0.0   \n",
       "\n",
       "   sqft_living15 basement  sqft_living15 viewed  sqft_lot15^2  \\\n",
       "0                     0.0                   0.0    59290000.0   \n",
       "1                  2370.0                   0.0    39476089.0   \n",
       "2                     0.0                3710.0    93799225.0   \n",
       "3                     0.0                4050.0   202379076.0   \n",
       "4                     0.0                   0.0    16402500.0   \n",
       "\n",
       "   sqft_lot15 sale_age  sqft_lot15 age  sqft_lot15 renovated  \\\n",
       "0             215600.0        215600.0                   0.0   \n",
       "1             232471.0        232471.0                   0.0   \n",
       "2             164645.0        164645.0                   0.0   \n",
       "3             355650.0        355650.0                   0.0   \n",
       "4              40500.0         40500.0                   0.0   \n",
       "\n",
       "   sqft_lot15 basement  sqft_lot15 viewed  sale_age^2  sale_age age  \\\n",
       "0                  0.0                0.0       784.0         784.0   \n",
       "1               6283.0                0.0      1369.0        1369.0   \n",
       "2                  0.0             9685.0       289.0         289.0   \n",
       "3                  0.0            14226.0       625.0         625.0   \n",
       "4                  0.0                0.0       100.0         100.0   \n",
       "\n",
       "   sale_age renovated  sale_age basement  sale_age viewed   age^2  \\\n",
       "0                 0.0                0.0              0.0   784.0   \n",
       "1                 0.0               37.0              0.0  1369.0   \n",
       "2                 0.0                0.0             17.0   289.0   \n",
       "3                 0.0                0.0             25.0   625.0   \n",
       "4                 0.0                0.0              0.0   100.0   \n",
       "\n",
       "   age renovated  age basement  age viewed  renovated^2  renovated basement  \\\n",
       "0            0.0           0.0         0.0          0.0                 0.0   \n",
       "1            0.0          37.0         0.0          0.0                 0.0   \n",
       "2            0.0           0.0        17.0          0.0                 0.0   \n",
       "3            0.0           0.0        25.0          0.0                 0.0   \n",
       "4            0.0           0.0         0.0          0.0                 0.0   \n",
       "\n",
       "   ...  bth_5.0  bth_5.25  bth_5.5  bth_5.75  bth_6.0  bth_6.25  bth_6.5  \\\n",
       "0  ...        0         0        0         0        0         0        0   \n",
       "1  ...        0         0        0         0        0         0        0   \n",
       "2  ...        0         0        0         0        0         0        0   \n",
       "3  ...        0         0        0         0        0         0        0   \n",
       "4  ...        0         0        0         0        0         0        0   \n",
       "\n",
       "   bth_6.75  bth_7.5  bth_7.75  bth_8.0  flr_1.5  flr_2.0  flr_2.5  flr_3.0  \\\n",
       "0         0        0         0        0        0        1        0        0   \n",
       "1         0        0         0        0        0        0        0        0   \n",
       "2         0        0         0        0        0        1        0        0   \n",
       "3         0        0         0        0        0        1        0        0   \n",
       "4         0        0         0        0        0        1        0        0   \n",
       "\n",
       "   flr_3.5  cnd_2  cnd_3  cnd_4  cnd_5  grd_3  grd_4  grd_5  grd_6  grd_7  \\\n",
       "0        0      0      0      1      0      0      0      0      0      0   \n",
       "1        0      0      0      0      1      0      0      0      0      0   \n",
       "2        0      0      1      0      0      0      0      0      0      0   \n",
       "3        0      0      1      0      0      0      0      0      0      0   \n",
       "4        0      0      1      0      0      0      0      0      0      0   \n",
       "\n",
       "   grd_8  grd_9  grd_10  grd_11  grd_12  grd_13  zip_98002  zip_98003  \\\n",
       "0      1      0       0       0       0       0          0          0   \n",
       "1      1      0       0       0       0       0          0          0   \n",
       "2      0      0       0       1       0       0          0          0   \n",
       "3      0      0       0       0       1       0          0          0   \n",
       "4      0      1       0       0       0       0          0          0   \n",
       "\n",
       "   zip_98004  zip_98005  zip_98006  zip_98007  zip_98008  zip_98010  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          1          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98011  zip_98014  zip_98019  zip_98022  zip_98023  zip_98024  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98027  zip_98028  zip_98029  zip_98030  zip_98031  zip_98032  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98033  zip_98034  zip_98038  zip_98039  zip_98040  zip_98042  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          1          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98045  zip_98052  zip_98053  zip_98055  zip_98056  zip_98058  \\\n",
       "0          0          0          0          0          0          1   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          1          0          0          0          0   \n",
       "\n",
       "   zip_98059  zip_98065  zip_98070  zip_98072  zip_98074  zip_98075  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98077  zip_98092  zip_98102  zip_98103  zip_98105  zip_98106  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98107  zip_98108  zip_98109  zip_98112  zip_98115  zip_98116  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          1          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98117  zip_98118  zip_98119  zip_98122  zip_98125  zip_98126  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98133  zip_98136  zip_98144  zip_98146  zip_98148  zip_98155  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98166  zip_98168  zip_98177  zip_98178  zip_98188  zip_98198  zip_98199  \n",
       "0          0          0          0          0          0          0          0  \n",
       "1          0          0          0          0          0          0          0  \n",
       "2          0          0          0          0          0          0          0  \n",
       "3          0          0          0          0          0          0          0  \n",
       "4          0          0          0          0          0          0          0  \n",
       "\n",
       "[5 rows x 233 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sqft_living</th>\n      <th>sqft_lot</th>\n      <th>waterfront</th>\n      <th>sqft_above</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>sqft_living15</th>\n      <th>sqft_lot15</th>\n      <th>sale_age</th>\n      <th>age</th>\n      <th>renovated</th>\n      <th>basement</th>\n      <th>viewed</th>\n      <th>sqft_living^2</th>\n      <th>sqft_living sqft_lot</th>\n      <th>sqft_living waterfront</th>\n      <th>sqft_living sqft_above</th>\n      <th>sqft_living lat</th>\n      <th>sqft_living long</th>\n      <th>sqft_living sqft_living15</th>\n      <th>sqft_living sqft_lot15</th>\n      <th>sqft_living sale_age</th>\n      <th>sqft_living age</th>\n      <th>sqft_living renovated</th>\n      <th>sqft_living basement</th>\n      <th>sqft_living viewed</th>\n      <th>sqft_lot^2</th>\n      <th>sqft_lot waterfront</th>\n      <th>sqft_lot sqft_above</th>\n      <th>sqft_lot lat</th>\n      <th>sqft_lot long</th>\n      <th>sqft_lot sqft_living15</th>\n      <th>sqft_lot sqft_lot15</th>\n      <th>sqft_lot sale_age</th>\n      <th>sqft_lot age</th>\n      <th>sqft_lot renovated</th>\n      <th>sqft_lot basement</th>\n      <th>sqft_lot viewed</th>\n      <th>waterfront^2</th>\n      <th>waterfront sqft_above</th>\n      <th>waterfront lat</th>\n      <th>waterfront long</th>\n      <th>waterfront sqft_living15</th>\n      <th>waterfront sqft_lot15</th>\n      <th>waterfront sale_age</th>\n      <th>waterfront age</th>\n      <th>waterfront renovated</th>\n      <th>waterfront basement</th>\n      <th>waterfront viewed</th>\n      <th>sqft_above^2</th>\n      <th>sqft_above lat</th>\n      <th>sqft_above long</th>\n      <th>sqft_above sqft_living15</th>\n      <th>sqft_above sqft_lot15</th>\n      <th>sqft_above sale_age</th>\n      <th>sqft_above age</th>\n      <th>sqft_above renovated</th>\n      <th>sqft_above basement</th>\n      <th>sqft_above viewed</th>\n      <th>lat^2</th>\n      <th>lat long</th>\n      <th>lat sqft_living15</th>\n      <th>lat sqft_lot15</th>\n      <th>lat sale_age</th>\n      <th>lat age</th>\n      <th>lat renovated</th>\n      <th>lat basement</th>\n      <th>lat viewed</th>\n      <th>long^2</th>\n      <th>long sqft_living15</th>\n      <th>long sqft_lot15</th>\n      <th>long sale_age</th>\n      <th>long age</th>\n      <th>long renovated</th>\n      <th>long basement</th>\n      <th>long viewed</th>\n      <th>sqft_living15^2</th>\n      <th>sqft_living15 sqft_lot15</th>\n      <th>sqft_living15 sale_age</th>\n      <th>sqft_living15 age</th>\n      <th>sqft_living15 renovated</th>\n      <th>sqft_living15 basement</th>\n      <th>sqft_living15 viewed</th>\n      <th>sqft_lot15^2</th>\n      <th>sqft_lot15 sale_age</th>\n      <th>sqft_lot15 age</th>\n      <th>sqft_lot15 renovated</th>\n      <th>sqft_lot15 basement</th>\n      <th>sqft_lot15 viewed</th>\n      <th>sale_age^2</th>\n      <th>sale_age age</th>\n      <th>sale_age renovated</th>\n      <th>sale_age basement</th>\n      <th>sale_age viewed</th>\n      <th>age^2</th>\n      <th>age renovated</th>\n      <th>age basement</th>\n      <th>age viewed</th>\n      <th>renovated^2</th>\n      <th>renovated basement</th>\n      <th>...</th>\n      <th>bth_5.0</th>\n      <th>bth_5.25</th>\n      <th>bth_5.5</th>\n      <th>bth_5.75</th>\n      <th>bth_6.0</th>\n      <th>bth_6.25</th>\n      <th>bth_6.5</th>\n      <th>bth_6.75</th>\n      <th>bth_7.5</th>\n      <th>bth_7.75</th>\n      <th>bth_8.0</th>\n      <th>flr_1.5</th>\n      <th>flr_2.0</th>\n      <th>flr_2.5</th>\n      <th>flr_3.0</th>\n      <th>flr_3.5</th>\n      <th>cnd_2</th>\n      <th>cnd_3</th>\n      <th>cnd_4</th>\n      <th>cnd_5</th>\n      <th>grd_3</th>\n      <th>grd_4</th>\n      <th>grd_5</th>\n      <th>grd_6</th>\n      <th>grd_7</th>\n      <th>grd_8</th>\n      <th>grd_9</th>\n      <th>grd_10</th>\n      <th>grd_11</th>\n      <th>grd_12</th>\n      <th>grd_13</th>\n      <th>zip_98002</th>\n      <th>zip_98003</th>\n      <th>zip_98004</th>\n      <th>zip_98005</th>\n      <th>zip_98006</th>\n      <th>zip_98007</th>\n      <th>zip_98008</th>\n      <th>zip_98010</th>\n      <th>zip_98011</th>\n      <th>zip_98014</th>\n      <th>zip_98019</th>\n      <th>zip_98022</th>\n      <th>zip_98023</th>\n      <th>zip_98024</th>\n      <th>zip_98027</th>\n      <th>zip_98028</th>\n      <th>zip_98029</th>\n      <th>zip_98030</th>\n      <th>zip_98031</th>\n      <th>zip_98032</th>\n      <th>zip_98033</th>\n      <th>zip_98034</th>\n      <th>zip_98038</th>\n      <th>zip_98039</th>\n      <th>zip_98040</th>\n      <th>zip_98042</th>\n      <th>zip_98045</th>\n      <th>zip_98052</th>\n      <th>zip_98053</th>\n      <th>zip_98055</th>\n      <th>zip_98056</th>\n      <th>zip_98058</th>\n      <th>zip_98059</th>\n      <th>zip_98065</th>\n      <th>zip_98070</th>\n      <th>zip_98072</th>\n      <th>zip_98074</th>\n      <th>zip_98075</th>\n      <th>zip_98077</th>\n      <th>zip_98092</th>\n      <th>zip_98102</th>\n      <th>zip_98103</th>\n      <th>zip_98105</th>\n      <th>zip_98106</th>\n      <th>zip_98107</th>\n      <th>zip_98108</th>\n      <th>zip_98109</th>\n      <th>zip_98112</th>\n      <th>zip_98115</th>\n      <th>zip_98116</th>\n      <th>zip_98117</th>\n      <th>zip_98118</th>\n      <th>zip_98119</th>\n      <th>zip_98122</th>\n      <th>zip_98125</th>\n      <th>zip_98126</th>\n      <th>zip_98133</th>\n      <th>zip_98136</th>\n      <th>zip_98144</th>\n      <th>zip_98146</th>\n      <th>zip_98148</th>\n      <th>zip_98155</th>\n      <th>zip_98166</th>\n      <th>zip_98168</th>\n      <th>zip_98177</th>\n      <th>zip_98178</th>\n      <th>zip_98188</th>\n      <th>zip_98198</th>\n      <th>zip_98199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2070.0</td>\n      <td>8893.0</td>\n      <td>0.0</td>\n      <td>2070.0</td>\n      <td>47.4388</td>\n      <td>-122.162</td>\n      <td>2390.0</td>\n      <td>7700.0</td>\n      <td>28.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4284900.0</td>\n      <td>18408510.0</td>\n      <td>0.0</td>\n      <td>4284900.0</td>\n      <td>98198.316</td>\n      <td>-252875.34</td>\n      <td>4947300.0</td>\n      <td>15939000.0</td>\n      <td>57960.0</td>\n      <td>57960.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>79085449.0</td>\n      <td>0.0</td>\n      <td>18408510.0</td>\n      <td>421873.2484</td>\n      <td>-1086386.666</td>\n      <td>21254270.0</td>\n      <td>68476100.0</td>\n      <td>249004.0</td>\n      <td>249004.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4284900.0</td>\n      <td>98198.316</td>\n      <td>-252875.34</td>\n      <td>4947300.0</td>\n      <td>15939000.0</td>\n      <td>57960.0</td>\n      <td>57960.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2250.439745</td>\n      <td>-5795.218686</td>\n      <td>113378.732</td>\n      <td>365278.7600</td>\n      <td>1328.2864</td>\n      <td>1328.2864</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>14923.554244</td>\n      <td>-291967.18</td>\n      <td>-940647.400</td>\n      <td>-3420.536</td>\n      <td>-3420.536</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>-0.000</td>\n      <td>5712100.0</td>\n      <td>18403000.0</td>\n      <td>66920.0</td>\n      <td>66920.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>59290000.0</td>\n      <td>215600.0</td>\n      <td>215600.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>784.0</td>\n      <td>784.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>784.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2900.0</td>\n      <td>6730.0</td>\n      <td>0.0</td>\n      <td>1830.0</td>\n      <td>47.6784</td>\n      <td>-122.285</td>\n      <td>2370.0</td>\n      <td>6283.0</td>\n      <td>37.0</td>\n      <td>37.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>8410000.0</td>\n      <td>19517000.0</td>\n      <td>0.0</td>\n      <td>5307000.0</td>\n      <td>138267.360</td>\n      <td>-354626.50</td>\n      <td>6873000.0</td>\n      <td>18220700.0</td>\n      <td>107300.0</td>\n      <td>107300.0</td>\n      <td>0.0</td>\n      <td>2900.0</td>\n      <td>0.0</td>\n      <td>45292900.0</td>\n      <td>0.0</td>\n      <td>12315900.0</td>\n      <td>320875.6320</td>\n      <td>-822978.050</td>\n      <td>15950100.0</td>\n      <td>42284590.0</td>\n      <td>249010.0</td>\n      <td>249010.0</td>\n      <td>0.0</td>\n      <td>6730.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3348900.0</td>\n      <td>87251.472</td>\n      <td>-223781.55</td>\n      <td>4337100.0</td>\n      <td>11497890.0</td>\n      <td>67710.0</td>\n      <td>67710.0</td>\n      <td>0.0</td>\n      <td>1830.0</td>\n      <td>0.0</td>\n      <td>2273.229827</td>\n      <td>-5830.353144</td>\n      <td>112997.808</td>\n      <td>299563.3872</td>\n      <td>1764.1008</td>\n      <td>1764.1008</td>\n      <td>0.0</td>\n      <td>47.6784</td>\n      <td>0.0000</td>\n      <td>14953.621225</td>\n      <td>-289815.45</td>\n      <td>-768316.655</td>\n      <td>-4524.545</td>\n      <td>-4524.545</td>\n      <td>-0.0</td>\n      <td>-122.285</td>\n      <td>-0.000</td>\n      <td>5616900.0</td>\n      <td>14890710.0</td>\n      <td>87690.0</td>\n      <td>87690.0</td>\n      <td>0.0</td>\n      <td>2370.0</td>\n      <td>0.0</td>\n      <td>39476089.0</td>\n      <td>232471.0</td>\n      <td>232471.0</td>\n      <td>0.0</td>\n      <td>6283.0</td>\n      <td>0.0</td>\n      <td>1369.0</td>\n      <td>1369.0</td>\n      <td>0.0</td>\n      <td>37.0</td>\n      <td>0.0</td>\n      <td>1369.0</td>\n      <td>0.0</td>\n      <td>37.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3770.0</td>\n      <td>10893.0</td>\n      <td>0.0</td>\n      <td>3770.0</td>\n      <td>47.5646</td>\n      <td>-122.129</td>\n      <td>3710.0</td>\n      <td>9685.0</td>\n      <td>17.0</td>\n      <td>17.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>14212900.0</td>\n      <td>41066610.0</td>\n      <td>0.0</td>\n      <td>14212900.0</td>\n      <td>179318.542</td>\n      <td>-460426.33</td>\n      <td>13986700.0</td>\n      <td>36512450.0</td>\n      <td>64090.0</td>\n      <td>64090.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3770.0</td>\n      <td>118657449.0</td>\n      <td>0.0</td>\n      <td>41066610.0</td>\n      <td>518121.1878</td>\n      <td>-1330351.197</td>\n      <td>40413030.0</td>\n      <td>105498705.0</td>\n      <td>185181.0</td>\n      <td>185181.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>10893.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14212900.0</td>\n      <td>179318.542</td>\n      <td>-460426.33</td>\n      <td>13986700.0</td>\n      <td>36512450.0</td>\n      <td>64090.0</td>\n      <td>64090.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3770.0</td>\n      <td>2262.391173</td>\n      <td>-5809.017033</td>\n      <td>176464.666</td>\n      <td>460663.1510</td>\n      <td>808.5982</td>\n      <td>808.5982</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>47.5646</td>\n      <td>14915.492641</td>\n      <td>-453098.59</td>\n      <td>-1182819.365</td>\n      <td>-2076.193</td>\n      <td>-2076.193</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>-122.129</td>\n      <td>13764100.0</td>\n      <td>35931350.0</td>\n      <td>63070.0</td>\n      <td>63070.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3710.0</td>\n      <td>93799225.0</td>\n      <td>164645.0</td>\n      <td>164645.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9685.0</td>\n      <td>289.0</td>\n      <td>289.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>17.0</td>\n      <td>289.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>17.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4560.0</td>\n      <td>14608.0</td>\n      <td>0.0</td>\n      <td>4560.0</td>\n      <td>47.6995</td>\n      <td>-122.228</td>\n      <td>4050.0</td>\n      <td>14226.0</td>\n      <td>25.0</td>\n      <td>25.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>20793600.0</td>\n      <td>66612480.0</td>\n      <td>0.0</td>\n      <td>20793600.0</td>\n      <td>217509.720</td>\n      <td>-557359.68</td>\n      <td>18468000.0</td>\n      <td>64870560.0</td>\n      <td>114000.0</td>\n      <td>114000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4560.0</td>\n      <td>213393664.0</td>\n      <td>0.0</td>\n      <td>66612480.0</td>\n      <td>696794.2960</td>\n      <td>-1785506.624</td>\n      <td>59162400.0</td>\n      <td>207813408.0</td>\n      <td>365200.0</td>\n      <td>365200.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14608.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>20793600.0</td>\n      <td>217509.720</td>\n      <td>-557359.68</td>\n      <td>18468000.0</td>\n      <td>64870560.0</td>\n      <td>114000.0</td>\n      <td>114000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4560.0</td>\n      <td>2275.242300</td>\n      <td>-5830.214486</td>\n      <td>193182.975</td>\n      <td>678573.0870</td>\n      <td>1192.4875</td>\n      <td>1192.4875</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>47.6995</td>\n      <td>14939.683984</td>\n      <td>-495023.40</td>\n      <td>-1738815.528</td>\n      <td>-3055.700</td>\n      <td>-3055.700</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>-122.228</td>\n      <td>16402500.0</td>\n      <td>57615300.0</td>\n      <td>101250.0</td>\n      <td>101250.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4050.0</td>\n      <td>202379076.0</td>\n      <td>355650.0</td>\n      <td>355650.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14226.0</td>\n      <td>625.0</td>\n      <td>625.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>25.0</td>\n      <td>625.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>25.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2550.0</td>\n      <td>5376.0</td>\n      <td>0.0</td>\n      <td>2550.0</td>\n      <td>47.6647</td>\n      <td>-122.083</td>\n      <td>2250.0</td>\n      <td>4050.0</td>\n      <td>10.0</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6502500.0</td>\n      <td>13708800.0</td>\n      <td>0.0</td>\n      <td>6502500.0</td>\n      <td>121544.985</td>\n      <td>-311311.65</td>\n      <td>5737500.0</td>\n      <td>10327500.0</td>\n      <td>25500.0</td>\n      <td>25500.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>28901376.0</td>\n      <td>0.0</td>\n      <td>13708800.0</td>\n      <td>256245.4272</td>\n      <td>-656318.208</td>\n      <td>12096000.0</td>\n      <td>21772800.0</td>\n      <td>53760.0</td>\n      <td>53760.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6502500.0</td>\n      <td>121544.985</td>\n      <td>-311311.65</td>\n      <td>5737500.0</td>\n      <td>10327500.0</td>\n      <td>25500.0</td>\n      <td>25500.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2271.923626</td>\n      <td>-5819.049570</td>\n      <td>107245.575</td>\n      <td>193042.0350</td>\n      <td>476.6470</td>\n      <td>476.6470</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>14904.258889</td>\n      <td>-274686.75</td>\n      <td>-494436.150</td>\n      <td>-1220.830</td>\n      <td>-1220.830</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>-0.000</td>\n      <td>5062500.0</td>\n      <td>9112500.0</td>\n      <td>22500.0</td>\n      <td>22500.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>16402500.0</td>\n      <td>40500.0</td>\n      <td>40500.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>100.0</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 233 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Grab columns for polynominal and interaction features from the original dataframe without dummy variables\n",
    "poly_feat = df.drop(['price', 'bedrooms', 'bathrooms', 'floors', 'condition', 'grade', 'zipcode'], axis=1)\n",
    "y = df['price']\n",
    "# Use PolynomialFeatures to create binomial and interaction features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_data = poly.fit_transform(poly_feat)\n",
    "poly_columns = poly.get_feature_names(poly_feat.columns)\n",
    "df_poly = pd.DataFrame(poly_data, columns=poly_columns)\n",
    "# Concatenating two dataframes together for input into linear regression model\n",
    "X = pd.concat([df_poly, df_dum], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "source": [
    "# Instantiating and Fitting a Supervised Learning Model \n",
    "\n",
    "There are several techniques we could use to prepare a linear regression model, including taking it to pen and paper to calculate means, standard deviations, correlations, and covariance.  Here we employ OLS method or ordinary least squares method.\n",
    "\n",
    "R^2 or the coefficient of determination is a measure to assess the goodness of fit of a regression model:\n",
    "\n",
    "$$ R^2 = 1 - \\frac{SS_{RES}}{SS_{TOT}} = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "- The R-squared value is more specifically the amount of variance in the dependent variable that can be explained by the independent variables, also the covariance of X and Y (SSxy) divided by variance in X (SSxx).\n",
    "- The intercept is the mean of the dependent variable when all the indepedent variables are zero. \n",
    "- The coefficients are estimates of the actual population parameters, where the increase\n",
    "\n",
    "- R-values range from 0 to 1, and higher values of R^2 would be indicative of a good fit, but if the R^2 value is too high, that often can be indicative of over-fitting. \n",
    "- The model starts becoming attuned to fit the noise in the sample rather than reflecting the entire population, which decreases its capability to make precise predictions.\n",
    "- \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Intercept : -3804210354.986055\nR^2 Score :  0.8797519008528518\nCoefficients : [-1.49291857e+04  4.75406407e+01  3.94448372e+07 -2.59759303e+04\n  1.41288975e+08 -7.81019356e+06 -1.04595256e+04  1.30273015e+02\n -2.10502368e+05 -2.04893173e+05 -1.45145314e+06  4.74925866e+06\n -1.22026469e+07 -8.88278973e-03 -3.61393976e-04  2.74244013e+02\n  3.35891471e-02  1.57090682e+02 -2.24569896e+02  4.21863761e-02\n -1.12054506e-03 -3.60556721e+00  2.72893588e+00 -9.15059039e+01\n -1.99579299e+04 -2.06220885e+01 -4.00472469e-07  9.86384652e-01\n  2.69431350e-04  2.10866494e-01  4.65852178e-01  9.97489500e-05\n  1.02137081e-06  1.07714646e-02 -1.25576453e-02  9.76441460e-01\n  1.88665614e-02  3.42700481e-02  4.02034066e+07 -7.51815051e+01\n  1.10313822e+06  1.40637453e+06 -8.38103035e+01 -3.42388412e-01\n -4.81472988e+03  4.54658377e+03 -2.79998877e+05 -1.40183302e+05\n  4.02034056e+07 -1.06802308e-02  9.18909824e+01 -1.36381598e+01\n -2.89016161e-02  9.72065143e-04  1.50038569e+00 -5.62201155e-01\n -1.15311459e+01  1.99631824e+04  4.50514878e+01 -1.21668218e+06\n  2.12698270e+05  2.62160444e+01 -1.21489224e+00 -1.88359863e+02\n  7.15264129e+02 -7.43950380e+04 -2.50797629e+04  1.61700207e+05\n  7.92708829e+03 -7.49478402e+01  5.83314328e-01 -1.86876270e+03\n -1.29846357e+03 -5.48177328e+04  6.85016756e+04 -1.36339550e+05\n  3.70943826e-03 -2.78702006e-04 -3.33063528e-02  8.88914473e-01\n  5.28804421e+01 -4.74989935e+00  2.39688816e+01 -9.23871994e-07\n -2.16701552e-02  1.25821177e-02 -1.73777846e+00  9.71959963e-01\n -1.56783998e-01  2.53209442e+00 -1.83267436e+01 -6.21614598e+03\n  3.25389533e+03 -2.67300676e+02  2.82592284e+01  3.16022005e+03\n -2.14657657e+03  4.99750702e+02 -1.45980207e+06  8.08764892e+04\n  6.83605044e+02  4.75748057e+06  3.65632611e+04 -1.22010464e+07\n  1.12392709e+05  9.28794011e+04  9.63643696e+04  9.61515287e+04\n  7.94916446e+04  2.22052143e+04 -1.57705306e+05 -3.10088871e+04\n -1.95955757e+05 -2.44325968e+05  2.59961444e+01 -7.22753551e+04\n -7.53041647e+04 -6.05522400e+03 -1.15450358e+05 -1.29955209e+04\n -9.65230801e+03 -1.31711011e+04  5.82716280e+03  5.75437854e+03\n  6.25760810e+03  2.30248194e+04  3.84135593e+04  2.98407616e+04\n  7.60132465e+04  7.00958742e+04  1.76596382e+05  8.48745526e+04\n  2.09436920e+05  2.92671911e+05  1.58597569e+05  2.05163908e+05\n  3.14568352e+05  7.13534304e+05  3.17518409e+05  2.17976125e+05\n -5.56852365e+05 -1.12543837e+05  1.91036540e+06  7.33674728e+05\n -1.66368731e+04 -3.29626059e+04  1.76914445e+04 -8.95878759e+04\n -1.79028707e+04  6.68764297e+04  8.77749521e+04  1.16238833e+05\n  1.61073026e+05 -2.33602641e+04  1.24460503e+05  5.30067196e+04\n  3.60431788e+04  5.35359853e+04  8.51151073e+04  1.59101245e+05\n  2.51980774e+05  4.04216487e+05  6.46527443e+05  1.05063202e+06\n  1.28385247e+04 -2.45133545e+04  6.83149342e+05  2.39306735e+05\n  2.05399660e+05  2.15868473e+05  2.41310279e+05  1.57820801e+05\n  1.08597917e+05  9.77877588e+04  1.21842877e+05  1.80473280e+05\n -2.74321658e+04  2.33012563e+05  1.97776368e+05  8.10036527e+04\n  2.45031709e+05 -3.34048732e+03 -1.77861445e+04 -1.84190408e+01\n  3.00447667e+05  1.63183753e+05  1.17708105e+05  1.01155307e+06\n  4.06935465e+05 -3.23400815e+03  1.86432565e+05  2.32351294e+05\n  1.77079541e+05 -1.75361659e+04  4.58105190e+04  3.82895045e+04\n  5.40870895e+04  1.85198213e+05  1.76536500e+04  1.23728879e+05\n  2.00575637e+05  2.00600915e+05  9.40877733e+04  2.55951557e+04\n  3.83911708e+05  2.61926767e+05  3.93275822e+05  4.98150926e+04\n  2.87066059e+05  4.79196855e+04  3.89323960e+05  4.82151187e+05\n  2.30637258e+05  1.86152596e+05  2.35055046e+05  9.43956016e+04\n  4.14665644e+05  2.70555989e+05  1.32132730e+05  8.96878424e+04\n  8.87035147e+04  1.69734543e+05  1.68070991e+05  2.40922194e+04\n  8.11172681e+04  1.01868204e+05 -4.86320088e+02 -3.57407662e+04\n  1.38356897e+05 -3.44850945e+04 -1.94080291e+03 -3.20515711e+04\n  2.64334303e+05]\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to instantiate a linear regression object and fit the model to the data\n",
    "lm = LinearRegression().fit(X, y)\n",
    "# We use the value of R_squared as an indication of the fit \n",
    "print('Intercept :', lm.intercept_)\n",
    "print('R^2 Score : ', lm.score(X, y))\n",
    "print('Coefficients :', lm.coef_)"
   ]
  },
  {
   "source": [
    "## **Train-Test Split**\n",
    "\n",
    "The train-test split is a technique for evaluating the performance of a machine learning algorithm, which can be used for classification or regression problems or any supervised learning algorithm.\n",
    "\n",
    "The procedure involves taking a dataset and dividing it into two subsets. The first subset is used to fit the model, the training dataset, and for the second subset, the test dataset, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values.  The objective is to estimate the performance of the machine learning model on new data: data not used to train the model.\n",
    "\n",
    "This is how we expect to use the model in practice. Namely, to fit it on available data with known inputs and outputs, then make predictions on new examples in the future where we do not have the expected output or target values.\n",
    "\n",
    "The train-test procedure is appropriate when there is a sufficiently large dataset available. When the dataset available is small, we can consider using a k-fold cross-validation procedure to evaluate the model performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R^2 Score:  0.8835608661235953\nIntercept:  -4019673483.164242\n"
     ]
    }
   ],
   "source": [
    "def bias(y, y_hat):\n",
    "\treturn np.mean(y_hat - y)\n",
    "def variance(y_hat):\n",
    "\treturn np.mean([yi**2 for yi in y_hat]) - np.mean(y_hat)**2"
   ]
  },
  {
   "source": [
    "### Assessing Training Model Performance, Predicting on Testing Set, and Comparing Model Performance\n",
    "\n",
    "As for what we use for regression metrics, \n",
    "\n",
    "1. MAE describes the typical magnitude of the residuals, where small MAE suggests that the model is good for prediction.\n",
    "2. MSE is the square of the difference between actual and predicted values, and will always be larger than MAE.  The presence of outliers will contribute quadratically to the error such that large differences between actual and predicted values are punished to a greater degree.\n",
    "3. RMSE is the square root of the variance of the residuals, which indicates the best absolute fit of the model to the data, having the same units as the target variable, where lower values indicates a better fit.  \n",
    "\n",
    "RMSE is the most important criterion for fit when we are working within prediction models and is the metric we most often use to compare between the training and testing model performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.8836\nTraining MAE:  79265.07\nTraining MSE:  1.6408e+10\nTraining RMSE:  128094.52\nTraining Bias:  0.0\nTraining Variance:  1.2469e+11\n\nTesting R^2 Score:  0.8649\nTesting MAE:  79461.47\nTesting MSE:  1.7982e+10\nTesting RMSE:  134097.92\nTesting Bias:  -1796.49\nTesting Variance:  1.1053e+11\n"
     ]
    }
   ],
   "source": [
    "X_tr, X_tt, y_tr, y_tt = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "lm = LinearRegression().fit(X_tr, y_tr)\n",
    "y_tr_pred = lm.predict(X_tr)\n",
    "y_tt_pred = lm.predict(X_tt)\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr, y_tr_pred), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr, y_tr_pred), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr, y_tr_pred)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr, y_tr_pred)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr, y_tr_pred), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(y_tr_pred)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt, y_tt_pred), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt, y_tt_pred), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt, y_tt_pred)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt, y_tt_pred)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt, y_tt_pred), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(y_tt_pred)))"
   ]
  },
  {
   "source": [
    "**Initial thoughts:**\n",
    "\n",
    "**Comparing the training and testing RMSE, there is expected increase in RMSE from the training to the testing set.  The model still includes all the polynomial and interaction features, so it is definitely still overfit.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "## Check for the Linear Regression assumptions:  Normal distribution of residuals and homoscedasticity\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this defines residuals as the sample estimate of the error for each observation\n",
    "residuals = (y_tt - y_tt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This checks for the normal distribution of the residuals or error term.  By satisfying this assumption, you are able to generate more reliable confidence and prediction intervals.\n",
    "# plt.hist(residuals)\n",
    "# plt.savefig('images/residuals.png')"
   ]
  },
  {
   "source": [
    "<img src='images/residuals.png'>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We use residplot to check for heteroscedasticity, which is the case where the residuals have a non-constant variance\n",
    "# sns.residplot(y_tt_pred, y_test, lowess=True, color='g')\n",
    "# plt.savefig('images/residplot.png')"
   ]
  },
  {
   "source": [
    "<img src='images/residplot.png'>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Feature Selection\n",
    "\n",
    "There are three types of feature selection methods:  filter, wrapper, and embedded.  Filtering methods, like K-Best, approach the problem by estimating the validities of features through statistical tests (i.e. correlation coefficient, information gain, chi-squared test, f-test) to assign scoring to each feature, which are subsequently ranked and selected.  Here K-Best uses the f-test (`f_regression`) to compare the least square errors between the two models and checks if the difference is significant and returns the top 20 features.\n",
    "\n",
    "Filtering methods are computationally less expensive than wrapper methods since we are not training an actual model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## KBest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['sqft_living',\n",
       " 'sqft_above',\n",
       " 'sqft_living15',\n",
       " 'sqft_living^2',\n",
       " 'sqft_living sqft_above',\n",
       " 'sqft_living lat',\n",
       " 'sqft_living long',\n",
       " 'sqft_living sqft_living15',\n",
       " 'sqft_living basement',\n",
       " 'sqft_living viewed',\n",
       " 'sqft_above^2',\n",
       " 'sqft_above lat',\n",
       " 'sqft_above long',\n",
       " 'sqft_above sqft_living15',\n",
       " 'sqft_above basement',\n",
       " 'sqft_above viewed',\n",
       " 'lat sqft_living15',\n",
       " 'long sqft_living15',\n",
       " 'sqft_living15^2',\n",
       " 'sqft_living15 viewed']"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Instantiate SelectKBest object and fit training data where k is the number of features you want to select\n",
    "kbest = SelectKBest(f_regression, k=20).fit(X_tr, y_tr)\n",
    "sel_columns = X_tr.columns[kbest.get_support()]\n",
    "rem_columns = X_tr.columns[~kbest.get_support()]\n",
    "list(sel_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.6877\nTraining MAE:  130573.33\nTraining MSE:  4.4014e+10\nTraining RMSE:  209795.48\nTraining Bias:  -0.0\nTraining Variance:  9.6902e+10\n\nTesting R^2 Score:  0.6911\nTesting MAE:  126932.94\nTesting MSE:  4.1107e+10\nTesting RMSE:  202748.99\nTesting Bias:  -4699.59\nTesting Variance:  8.2792e+10\n"
     ]
    }
   ],
   "source": [
    "# Instantiate linear regression object and fit the linear regression to the data\n",
    "kb = LinearRegression().fit(X_tr[sel_columns], y_tr)\n",
    "y_kb_tr_pred = kb.predict(X_tr[sel_columns])\n",
    "kb_tr_rmse = np.sqrt(metrics.mean_squared_error(y_tr, y_tr_pred))\n",
    "y_kb_tt_pred = kb.predict(X_tt[sel_columns])\n",
    "\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr, y_kb_tr_pred), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr, y_kb_tr_pred), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr, y_kb_tr_pred)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr, y_kb_tr_pred)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr, y_kb_tr_pred), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(y_kb_tr_pred)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt, y_kb_tt_pred), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt, y_kb_tt_pred), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt, y_kb_tt_pred)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt, y_kb_tt_pred)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt, y_kb_tt_pred), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(y_kb_tt_pred)))"
   ]
  },
  {
   "source": [
    "## RFECV\n",
    "\n",
    "Wrapper algorithms like RFECV returns a best set of features with an extensive greedy search, where different combinations are prepared, evaluated, and compared to other combinations.  Recursive Feature Elimination and Cross-Validation Selection begins with a model with the complete set of predictors and a score is assigned to each predictor, and the least important are removed.  The model is then rebuilt, and importance scores are computed again.  It is usually best practice to identify multicollinearity first, as it will select relevant and redundant features alike. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal number of features : 226\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "rfe = RFECV(estimator=lm, step=1, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "rfe.fit(X_tr, y_tr)\n",
    "selected = X_tr.columns[rfe.support_]\n",
    "print(\"Optimal number of features : %d\" % rfe.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.8842\nTraining MAE:  78848.97\nTraining MSE:  1.6312e+10\nTraining RMSE:  127719.34\nTraining Bias:  -0.0\nTraining Variance:  1.2460e+11\n\nTesting R^2 Score:  0.8659\nTesting MAE:  78660.89\nTesting MSE:  1.7841e+10\nTesting RMSE:  133569.89\nTesting Bias:  -1884.29\nTesting Variance:  1.1037e+11\n"
     ]
    }
   ],
   "source": [
    "rfecv = LinearRegression().fit(X_tr[selected], y_tr)\n",
    "y_tr_pred_rfe = rfecv.predict(X_tr[selected])\n",
    "y_tt_pred_rfe = rfecv.predict(X_tt[selected])\n",
    "\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr, y_tr_pred_rfe), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr, y_tr_pred_rfe), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr, y_tr_pred_rfe)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr, y_tr_pred_rfe)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr, y_tr_pred_rfe), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(y_tr_pred_rfe)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt, y_tt_pred_rfe), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt, y_tt_pred_rfe), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt, y_tt_pred_rfe)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt, y_tt_pred_rfe)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt, y_tt_pred_rfe), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(y_tt_pred_rfe)))"
   ]
  },
  {
   "source": [
    "# Second Run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['price']\n",
    "X = df.drop(columns=['price'], axis=1)\n",
    "X_tr2, X_tt2, y_tr2, y_tt2 = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "feat_cat = df[['condition', 'grade', 'waterfront', 'floors', 'bedrooms', 'bathrooms', 'zipcode', 'renovated', 'basement', 'viewed']]\n",
    "col_cat = feat_cat.columns\n",
    "feat_cont = df[['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'sqft_lot15', 'sale_age', 'age', 'lat', 'long']]\n",
    "col_cont = feat_cont.columns\n",
    "X_train_cont = X_tr2.loc[:, col_cont]\n",
    "X_test_cont = X_tt2.loc[:, col_cont]\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train_cont)\n",
    "X_test_scaled = ss.fit_transform(X_test_cont)\n",
    "cont_tr_df = pd.DataFrame(X_train_scaled, columns=X_train_cont.columns)\n",
    "cont_tt_df = pd.DataFrame(X_test_scaled, columns=X_test_cont.columns)\n",
    "\n",
    "X_train_cat = X_tr2.loc[:, col_cat]\n",
    "X_test_cat = X_tt2.loc[:, col_cat]\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)\n",
    "columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "\n",
    "X_tr2 = pd.concat([cont_tr_df, cat_train_df], axis=1)\n",
    "X_tt2 = pd.concat([cont_tt_df, cat_test_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.8393\nTraining MAE:  89133.57\nTraining MSE:  2.2647e+10\nTraining RMSE:  150488.38\nTraining Bias:  8.12\nTraining Variance:  1.1827e+11\n\nTesting R^2 Score:  -1.2719867969339525e+18\nTesting MAE:  10480690123271.31\nTesting MSE:  1.6925e+29\nTesting RMSE:  411395702718068.6\nTesting Bias:  10480690041742.34\nTesting Variance:  1.6914e+29\n"
     ]
    }
   ],
   "source": [
    "lm2 = LinearRegression().fit(X_tr2, y_tr2)\n",
    "y_tr_pred2 = lm2.predict(X_tr2)\n",
    "y_tt_pred2 = lm2.predict(X_tt2)\n",
    "\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr2, y_tr_pred2), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr2, y_tr_pred2), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr2, y_tr_pred2)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr2, y_tr_pred2)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr2, y_tr_pred2), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(y_tr_pred2)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt2, y_tt_pred2), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt2, y_tt_pred2), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt2, y_tt_pred2)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt2, y_tt_pred2)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt2, y_tt_pred2), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(y_tt_pred2)))"
   ]
  },
  {
   "source": [
    "## Ridge and Lasso Regression\n",
    "\n",
    "Embedded methods learn which features best contribute to the accuracy of the model while the model is being created, the most common being the regularization methods.  They are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm that bias the model toward lower complexity, i.e. fewer coefficients.\n",
    "\n",
    "Ridge regression optimizes the RSS by adding a penalty equivalent to the square of the magnitude of the coefficients, while Lasso adds a penalty equivalent to the absolute value of the magnitude of the coefficients."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.8383\nTraining MAE:  89614.14\nTraining MSE:  2.2782e+10\nTraining RMSE:  150938.29\nTraining Bias:  -0.0\nTraining Variance:  1.1698e+11\n\nTesting R^2 Score:  0.8402\nTesting MAE:  86599.43\nTesting MSE:  2.1257e+10\nTesting RMSE:  145799.13\nTesting Bias:  5205.19\nTesting Variance:  1.1420e+11\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=1).fit(X_tr2, y_tr2)\n",
    "y_ridge_tr = ridge.predict(X_tr2)\n",
    "y_ridge_tt = ridge.predict(X_tt2)\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr2, y_ridge_tr), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr2, y_ridge_tr), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr2, y_ridge_tr)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr2, y_ridge_tr)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr2, y_ridge_tr), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(y_ridge_tr)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt2, y_ridge_tt), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt2, y_ridge_tt), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt2, y_ridge_tt)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt2, y_ridge_tt)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt2, y_ridge_tt), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(y_ridge_tt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.8393\nTraining MAE:  89144.08\nTraining MSE:  2.2647e+10\nTraining RMSE:  150489.09\nTraining Bias:  -0.0\nTraining Variance:  1.1822e+11\n\nTesting R^2 Score:  0.8404\nTesting MAE:  85971.41\nTesting MSE:  2.1237e+10\nTesting RMSE:  145730.86\nTesting Bias:  4413.75\nTesting Variance:  1.1343e+11\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=1).fit(X_tr2, y_tr2)\n",
    "y_lasso_tr = lasso.predict(X_tr2)\n",
    "y_lasso_tt = lasso.predict(X_tt2)\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr2, y_lasso_tr), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr2, y_lasso_tr), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr2, y_lasso_tr)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr2, y_lasso_tr)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr2, y_lasso_tr), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(y_lasso_tr)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt2, y_lasso_tt), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt2, y_lasso_tt), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt2, y_lasso_tt)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt2, y_lasso_tt)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt2, y_lasso_tt), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(y_lasso_tt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_rmse_lasso = []\n",
    "# tt_rmse_lasso = []\n",
    "# alphas_lasso = []\n",
    "\n",
    "# for alpha in np.linspace(0, 200, num=50):\n",
    "#     lasso = Lasso(alpha=alpha)\n",
    "#     lasso.fit(X_tr2, y_tr2)\n",
    "#     tr_pred = lasso.predict(X_tr2)\n",
    "#     tr_rmse_lasso.append(np.sqrt(mean_squared_error(y_tr2, tr_pred)))\n",
    "#     tt_pred = lasso.predict(X_tt2)\n",
    "#     tt_rmse_lasso.append(np.sqrt(mean_squared_error(y_tt2, tt_pred)))\n",
    "#     alphas_lasso.append(alpha)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(alphas_lasso, tr_rmse_lasso, label=\"Train\")\n",
    "# ax.plot(alphas_lasso, tt_rmse_lasso, label=\"Test\")\n",
    "# ax.set_xlabel(\"Alpha\")\n",
    "# ax.set_ylabel(\"RMSE\")\n",
    "# optimal_alpha = alphas_lasso[np.argmin(tt_rmse_lasso)]\n",
    "# ax.axvline(optimal_alpha, color=\"black\", linestyle=\"--\")\n",
    "# print(f'Optimal Alpha Value: {int(optimal_alpha)}')\n",
    "# plt.savefig('images/optimal_lasso.png')"
   ]
  },
  {
   "source": [
    "<img src=\"images/optimal_lasso.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.8313\nTraining MAE:  91025.94\nTraining MSE:  2.3773e+10\nTraining RMSE:  154186.13\nTraining Bias:  0.0\nTraining Variance:  1.1330e+11\n\nTesting R^2 Score:  0.8366\nTesting MAE:  87877.83\nTesting MSE:  2.1739e+10\nTesting RMSE:  147440.46\nTesting Bias:  5510.34\nTesting Variance:  1.1346e+11\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=163, normalize=False)\n",
    "lasso.fit(X_tr2, y_tr2)\n",
    "y_best_lasso_tr = lasso.predict(X_tr2)\n",
    "y_best_lasso_tt = lasso.predict(X_tt2)\n",
    "\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr2, y_best_lasso_tr), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr2, y_best_lasso_tr), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr2, y_best_lasso_tr)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr2, y_best_lasso_tr)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr2, y_best_lasso_tr), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(y_best_lasso_tr)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt2, y_best_lasso_tt), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt2, y_best_lasso_tt), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt2, y_best_lasso_tt)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt2, y_best_lasso_tt)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt2, y_best_lasso_tt), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(y_best_lasso_tt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_rmse_ridge = []\n",
    "# tt_rmse_ridge = []\n",
    "# alphas_ridge = []\n",
    "\n",
    "# for alpha in np.linspace(0, 10, num=50):\n",
    "#     ridge = Ridge(alpha=alpha)\n",
    "#     ridge.fit(X_tr2, y_tr2)\n",
    "#     tr_pred = ridge.predict(X_tr2)\n",
    "#     tr_rmse_ridge.append(np.sqrt(mean_squared_error(y_tr2, tr_pred)))\n",
    "#     tt_pred = ridge.predict(X_tt2)\n",
    "#     tt_rmse_ridge.append(np.sqrt(mean_squared_error(y_tt2, tt_pred)))\n",
    "#     alphas_ridge.append(alpha)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(alphas_ridge, tr_rmse_ridge, label=\"Train\")\n",
    "# ax.plot(alphas_ridge, tt_rmse_ridge, label=\"Test\")\n",
    "# ax.set_xlabel(\"Alpha\")\n",
    "# ax.set_ylabel(\"RMSE\")\n",
    "# optimal_alpha = alphas_ridge[np.argmin(tt_rmse_ridge)]\n",
    "# ax.axvline(optimal_alpha, color=\"black\", linestyle=\"--\")\n",
    "# print(f'Optimal Alpha Value: {int(optimal_alpha)}')\n",
    "# plt.savefig('images/optimal_ridge.png')"
   ]
  },
  {
   "source": [
    "<img src=\"images/optimal_ridge.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.8383\nTraining MAE:  89614.14\nTraining MSE:  2.2782e+10\nTraining RMSE:  150938.29\nTraining Bias:  -0.0\nTraining Variance:  1.1698e+11\n\nTesting R^2 Score:  0.8402\nTesting MAE:  86599.43\nTesting MSE:  2.1257e+10\nTesting RMSE:  145799.13\nTesting Bias:  5205.19\nTesting Variance:  1.1420e+11\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=1, normalize=False)\n",
    "ridge.fit(X_tr2, y_tr2)\n",
    "y_ridge_tr = ridge.predict(X_tr2)\n",
    "y_ridge_tt = ridge.predict(X_tt2)\n",
    "\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr2, y_ridge_tr), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr2, y_ridge_tr), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr2, y_ridge_tr)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr2, y_ridge_tr)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr2, y_ridge_tr), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(y_ridge_tr)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt2, y_ridge_tt), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt2, y_ridge_tt), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt2, y_ridge_tt)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt2, y_ridge_tt)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt2, y_ridge_tt), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(y_ridge_tt)))"
   ]
  },
  {
   "source": [
    "## Decision Tree Regressor with GridSearchCV and Pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.9999\nTraining MAE:  169.94\nTraining MSE:  1.4119e+07\nTraining RMSE:  3757.56\nTraining Bias:  0.0\nTraining Variance:  1.4090e+11\n\nTesting R^2 Score:  0.7134\nTesting MAE:  106027.11\nTesting MSE:  3.8133e+10\nTesting RMSE:  195277.84\nTesting Bias:  8867.8\nTesting Variance:  1.3233e+11\n"
     ]
    }
   ],
   "source": [
    "dtm = DecisionTreeRegressor(random_state=42).fit(X_tr2, y_tr2)\n",
    "y_tr_pred3 = dtm.predict(X_tr2) \n",
    "y_tt_pred3 = dtm.predict(X_tt2)\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr2, y_tr_pred3), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr2, y_tr_pred3), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr2, y_tr_pred3)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr2, y_tr_pred3)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr2, y_tr_pred3), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(y_tr_pred3)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt2, y_tt_pred3), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt2, y_tt_pred3), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt2, y_tt_pred3)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt2, y_tt_pred3)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt2, y_tt_pred3), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(y_tt_pred3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n",
      "Training Score: -177947.23501758592\n",
      "Pipeline(steps=[('decisiontreeregressor',\n",
      "                 DecisionTreeRegressor(max_depth=8, max_leaf_nodes=100,\n",
      "                                       min_samples_leaf=5, min_samples_split=40,\n",
      "                                       random_state=1))])\n"
     ]
    }
   ],
   "source": [
    "rmse_scorer = make_scorer(root_mean_squared_error, greater_is_better=False)\n",
    "pipe_tree = make_pipeline(tree.DecisionTreeRegressor(random_state=1))\n",
    "dtm = DecisionTreeRegressor()\n",
    "param_grid = [{\n",
    "               'decisiontreeregressor__max_depth': [1, 2, 4, 6, 8],\n",
    "               'decisiontreeregressor__min_samples_leaf': [1, 5, 10, 20, 50, 100],\n",
    "               'decisiontreeregressor__max_leaf_nodes': [5, 20, 100],\n",
    "               'decisiontreeregressor__min_samples_split': [5, 10, 20, 40]\n",
    "             }]\n",
    "gs = GridSearchCV(pipe_tree, param_grid=param_grid, cv=10, scoring=rmse_scorer, verbose=1)\n",
    "gs.fit(X_tr2, y_tr2)\n",
    "print(f\"Training Score: {gs.best_score_}\")\n",
    "print(gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.8235\nTraining MAE:  91573.88\nTraining MSE:  2.4873e+10\nTraining RMSE:  157713.0\nTraining Bias:  -0.0\nTraining Variance:  1.1604e+11\n\nTesting R^2 Score:  0.7527\nTesting MAE:  98818.42\nTesting MSE:  3.2907e+10\nTesting RMSE:  181402.89\nTesting Bias:  8838.99\nTesting Variance:  1.1522e+11\n"
     ]
    }
   ],
   "source": [
    "gs_model = gs.best_estimator_\n",
    "gs_model.fit(X_tr2, y_tr2)\n",
    "gs_tr_pred = gs_model.predict(X_tr2)\n",
    "gs_tt_pred = gs_model.predict(X_tt2) \n",
    "\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr2, gs_tr_pred), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr2, gs_tr_pred), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr2, gs_tr_pred)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr2, gs_tr_pred)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr2, gs_tr_pred), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(gs_tr_pred)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt2, gs_tt_pred), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt2, gs_tt_pred), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt2, gs_tt_pred)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt2, gs_tt_pred)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt2, gs_tt_pred), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(gs_tt_pred)))"
   ]
  },
  {
   "source": [
    "## Random Forest Regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training R^2 Score:  0.9385\nTraining MAE:  45521.87\nTraining MSE:  8.6650e+09\nTraining RMSE:  93086.22\nTraining Bias:  -81.42\nTraining Variance:  1.1924e+11\n\nTesting R^2 Score:  0.8717\nTesting MAE:  72325.65\nTesting MSE:  1.7077e+10\nTesting RMSE:  130679.94\nTesting Bias:  5685.52\nTesting Variance:  1.1244e+11\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, \n",
    "                           max_features=\"auto\", \n",
    "                           max_depth=100, \n",
    "                           min_samples_leaf=4, \n",
    "                           min_samples_split=10, \n",
    "                           random_state=1).fit(X_tr2, y_tr2)\n",
    "rf_tr_pred = rf.predict(X_tr2)\n",
    "rf_tt_pred = rf.predict(X_tt2)\n",
    "\n",
    "print('Training R^2 Score: ', round(r2_score(y_tr2, rf_tr_pred), 4))\n",
    "print('Training MAE: ', round(mean_absolute_error(y_tr2, rf_tr_pred), 2))\n",
    "print('Training MSE: ', \"{:.4e}\".format(mean_squared_error(y_tr2, rf_tr_pred)))\n",
    "print('Training RMSE: ', round(np.sqrt(mean_squared_error(y_tr2, rf_tr_pred)), 2))\n",
    "print('Training Bias: ', round(bias(y_tr2, rf_tr_pred), 2))\n",
    "print('Training Variance: ', \"{:.4e}\".format(variance(rf_tr_pred)))\n",
    "print(\"\")\n",
    "print('Testing R^2 Score: ', round(r2_score(y_tt2, rf_tt_pred), 4))\n",
    "print('Testing MAE: ', round(mean_absolute_error(y_tt2, rf_tt_pred), 2))\n",
    "print('Testing MSE: ', \"{:.4e}\".format(mean_squared_error(y_tt2, rf_tt_pred)))\n",
    "print('Testing RMSE: ', round(np.sqrt(mean_squared_error(y_tt2, rf_tt_pred)), 2))\n",
    "print('Testing Bias: ', round(bias(y_tt2, rf_tt_pred), 2))\n",
    "print('Testing Variance: ', \"{:.4e}\".format(variance(rf_tt_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               importance\n",
       "sqft_living      0.575386\n",
       "lat              0.185811\n",
       "long             0.070759\n",
       "sqft_living15    0.040826\n",
       "sqft_above       0.021481\n",
       "waterfront_1     0.014275\n",
       "waterfront_0     0.012121\n",
       "sqft_lot         0.011179\n",
       "sqft_lot15       0.010704\n",
       "age              0.006921\n",
       "sale_age         0.006411\n",
       "grade_7          0.005232\n",
       "viewed_0         0.003663\n",
       "viewed_1         0.003539\n",
       "zipcode_98004    0.003254\n",
       "grade_8          0.003196\n",
       "grade_11         0.002894\n",
       "grade_10         0.002633\n",
       "grade_9          0.002449\n",
       "grade_12         0.001288"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sqft_living</th>\n      <td>0.575386</td>\n    </tr>\n    <tr>\n      <th>lat</th>\n      <td>0.185811</td>\n    </tr>\n    <tr>\n      <th>long</th>\n      <td>0.070759</td>\n    </tr>\n    <tr>\n      <th>sqft_living15</th>\n      <td>0.040826</td>\n    </tr>\n    <tr>\n      <th>sqft_above</th>\n      <td>0.021481</td>\n    </tr>\n    <tr>\n      <th>waterfront_1</th>\n      <td>0.014275</td>\n    </tr>\n    <tr>\n      <th>waterfront_0</th>\n      <td>0.012121</td>\n    </tr>\n    <tr>\n      <th>sqft_lot</th>\n      <td>0.011179</td>\n    </tr>\n    <tr>\n      <th>sqft_lot15</th>\n      <td>0.010704</td>\n    </tr>\n    <tr>\n      <th>age</th>\n      <td>0.006921</td>\n    </tr>\n    <tr>\n      <th>sale_age</th>\n      <td>0.006411</td>\n    </tr>\n    <tr>\n      <th>grade_7</th>\n      <td>0.005232</td>\n    </tr>\n    <tr>\n      <th>viewed_0</th>\n      <td>0.003663</td>\n    </tr>\n    <tr>\n      <th>viewed_1</th>\n      <td>0.003539</td>\n    </tr>\n    <tr>\n      <th>zipcode_98004</th>\n      <td>0.003254</td>\n    </tr>\n    <tr>\n      <th>grade_8</th>\n      <td>0.003196</td>\n    </tr>\n    <tr>\n      <th>grade_11</th>\n      <td>0.002894</td>\n    </tr>\n    <tr>\n      <th>grade_10</th>\n      <td>0.002633</td>\n    </tr>\n    <tr>\n      <th>grade_9</th>\n      <td>0.002449</td>\n    </tr>\n    <tr>\n      <th>grade_12</th>\n      <td>0.001288</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index=X_tr2.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a linear regression object\n",
    "lm_final = Lasso(alpha=163, normalize=False)\n",
    "lm_final.fit(X_tr2, y_tr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_final.coef_"
   ]
  },
  {
   "source": [
    "# Pickle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"model.pickle\",\"wb\")\n",
    "pickle.dump(lm_final, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"scaler.pickle\", \"wb\")\n",
    "pickle.dump(scaler, pickle_out)\n",
    "pickle_out.close"
   ]
  }
 ]
}